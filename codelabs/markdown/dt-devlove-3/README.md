summary: Why Developers Love Dynatrace - Episode 3
id: why-devs-love-dynatrace-3
categories: codelevel, cicd, qualitygates
tags: codelevel
status: Published
authors: sergio.hinojosa
Analytics Account: UA-193960361-1

# Why Devs Love Dynatrace - Episode 3


## Introduction 
Duration: 1

Positive
: [Register now for Why Devs Love Dynatrace ‚Äì Episode 3 ‚Äì Automated Release Comparison](https://info.dynatrace.com/global_all_wc_performance_clinic_why_devs_love_dynatrace_ep3_17116_registration.html)    
[Streaming live üì∫ on Thursday,Thursday, June 24th 4pm BST / 5pm CEST / 11am ET](https://info.dynatrace.com/global_all_wc_performance_clinic_why_devs_love_dynatrace_ep3_17116_registration.html) - Bring your questions üôã‚Äç‚ôÇÔ∏è


### Abstract of Episode 3
In the previous episodes you learned how Dynatrace automates observability from Dev to Ops environments, how to analyze distributed traces on real time (we call them PurePaths) to detect code or performance hotspots and how to automate code evaluation as part of your development process and delivery pipelines. You learned about automated performance validation and release comparison through intelligent quality gates and were introduced into the concepts of SLOs (Service Level Objectives).

On Episode 3 will learn the foundations of Load Test Analysis with Dynatrace. We will compare failed releases that did not pass the Quality Gate due performance degradations. We will show how easy it is to compare functional performance tests with each other learning how Dynatrace can pinpoint the degradations up to code-level through the application stack. We will even decompile the code to understand the root cause of a bad implementation whether it is a bad algorithm or thread synchronization issue.